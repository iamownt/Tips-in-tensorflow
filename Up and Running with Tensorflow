#auto set it the default session, but need sess.close()
sess = tf.InteractiveSession()

""'
that the reason why I use regularization hyperparameters:
l2_loss = np.sum(np.square(theta[1:]))
gradients = 1/m*X_train.T.dot(error)+np.r_[np.zeros((1, n_outputs)), alpha*theta[1:]]
"""
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(housing.data, housing.target.reshape(-1, 1))
print(np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])
[[-3.69419202e+01]
 [ 4.36693293e-01]
 [ 9.43577803e-03]
 [-1.07322041e-01]
 [ 6.45065694e-01]
 [-3.97638942e-06]
 [-3.78654265e-03]
 [-4.21314378e-01]
 [-4.34513755e-01]]
#less important
scaler = StandardScaler()
scaled_housing_data = scaler.fit_transform(housing.data)
scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]
X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name="X")
y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name="y")

theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name="theta")# pay attention to (n+1) dimension
gradients = 2/m * tf.matmul(tf.transpose(X), error)
training_op = tf.assign(theta, theta-learning_rate*gradients)
#the two lines above can be changed to
gradients = tf.gradients(mse, [theta])[0]
training_op = tf.assign(thetam theta-learning_rate*gradients)
#or
optimizer = tf.train.GradienDescentOptimizer(learning_rate=learning_rate)
training_op = optimizer.minimize(mse)

# a method for mini_batch GD
n_epochs = 20
batch_size = 100
n_batches = int(np.ceil(m / batch_size))
def fetch_batch(epoch, batch_index, batch_size):
    np.random.seed(epoch * n_batches + batch_index) 
    indices = np.random.randint(m, size=batch_size) # np.random.randint
    X_batch = scaled_housing_data_plus_bias[indices]
    y_batch = housing.target.reshape(-1, 1)[indices]
    return X_batch, y_batch

with tf.Session() as sess:
    sess.run(init)

    for epoch in range(n_epochs):
        for batch_index in range(n_batches):
            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)
            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})

    best_theta = theta.eval()
    

#save and restore models
saver = tf.train.Saver()#at the end of construction phase
...
if epochs%5==0:
    save_path = saver.save(sess,'/tmp/my_model.ckpt')
...
save_path = saver.save(sess, '/tmp/my_model.ckpt')# at the end of excution phase
!!! if use'/tmp/my_model.ckpt' the model will save in C:/tmp!!!
!!! if use'./tmp/my_model.ckpt' the model will save in $ ownt/tmp/!!!
#can restore the parameters
with tf.Session() as sess:
    saver.restore(sess, "/save_model/my_model_final.ckpt")
    best_theta_restored = theta.eval()

#by default the saver save the graph structure in the '.meta' file
reset_graph() # notice that we start with an empty graph.
saver = tf.train.import_meta_graph("/tmp/my_model_final.ckpt.meta")  # this loads the graph structure
theta = tf.get_default_graph().get_tensor_by_name("theta:0")
with tf.Session() as sess:
    saver.restore(sess, "/tmp/my_model_final.ckpt")  # this restores the graph's state
    best_theta_restored = theta.eval()

# some problems occur when using tensorboard
just change the demands to tensorboard --logdir=mylogs:'$...' or tensorboard --logdir=$

from datetime import datetime

now = datetime.utcnow().strftime("%Y%m%d%H%M%S")#year...
root_logdir = "tf_logs"
logdir = "{}/run-{}/".format(root_logdir, now)
#add the following two lines at the very end of the construction phase
mse_summary = tf.summary.scalar('MSE', mse)
'''
   create a node in the graph that will evaluate the MSE and write it 
   to the tensorboard-compatible binary log called a summary
'''
file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())
#create a FileWriter that you will use to write summaries to logfiles in the log directory

#change the excution phase to:
with tf.Session() as sess:                                                       
    sess.run(init)                                                                
    for epoch in range(n_epochs):                                                 
        for batch_index in range(n_batches):
            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)
            if batch_index % 10 == 0:#avoid slow training
                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})
                step = epoch * n_batches + batch_index
                file_writer.add_summary(summary_str, step)
            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
    best_theta = theta.eval()   
file_writer.flush()
'''
Flushes the event file to disk.
Call this method to make sure that all pending events have been written to disk.
'''
file_writer.close()

node.op.name

config = tf.ConfigProto()
config.gpu_options.allow_growth = True
with tf.Session(config=config) as sess:        #session = tf.Session(config=config) 
     ......

# to creat a relu function     
w_shape = (int(X.get_shape()[1]), 1)
tf.add_n([list], name='output)

!!!return tf.maximum(0, z, name='max')!!! if type(z) is tf.float32, 0 must be 0.0!!!

#a function can have a hasattr like: relu.threshold or you can use del relu.threshold to delete it
#relu is a function
if not hasattr(relu, threshold):
    relu.threshold = tf.Variable(0.0, name='threshold')
''' 
TensorFlow offers a solution which is a bit tricky to understand at first, but since it is used a lot in TensorFlowit is worth
going into a bit of detail. The idea is to use the get_variable() function to create the shared variable if it does not exist yet,
or reuse it if it already exists. The desired behavior (creating or reusing) is controlled by an attribute of the current
variable_scope(). For example, the following code will create a variable named "relu/threshold" 
(as a scalar, since shape=(), and using 0.0 as the initial value):
'''
reset_graph()

with tf.variable_scope("relu"):
    threshold = tf.get_variable("threshold", shape=(), initializer=tf.constant_initializer(0.0))
# reuse
with tf.variable_scope("relu", reuse=True):
    threshold = tf.get_variable("threshold")
# alternatively
with tf.variable_scope("relu") as scope:
    scope.reuse_variables()
    threshold = tf.get_variable("threshold")
    
# the scope.reuse_variables() is important
def relu(X):
    with tf.variable_scope("relu"):
        threshold = tf.get_variable("threshold", shape=(), initializer=tf.constant_initializer(0.0))
        w_shape = (int(X.get_shape()[1]), 1)
        w = tf.Variable(tf.random_normal(w_shape), name="weights")
        b = tf.Variable(0.0, name="bias")
        z = tf.add(tf.matmul(X, w), b, name="z")
        return tf.maximum(z, threshold, name="max")
X = tf.placeholder(tf.float32, shape=(None, n_features), name="X")
with tf.variable_scope("", default_name="") as scope:
    first_relu = relu(X)     # create the shared variable
    scope.reuse_variables()  # then reuse it
    relus = [first_relu] + [relu(X) for i in range(4)]
output = tf.add_n(relus, name="output")

#LR with mini-batch GD using tensorflow
#~True = False
y_pred = (y_proba_val>=0.5) #y_pred.shape == y_proba_val.shape
# sklearn.preprocessing.PolynomialFeatures(degrees=2, include_bias=False) if False delete 1
'''original features(a, b) ,combination number(1, a, b, a^2, b^2, ab)'''
#adding features like PolynomialFeatures
X_train_enhanced = np.c_[X_train, np.square(X_train[:, 1], X_train[:, 1]**3]

#copy classic!!! very beautiful codes!!!!!
from sklearn.datasets import make_moons

X_moons, y_moons = make_moons(1000, noise=0.1, random_state=42)
X_moons_with_bias = np.c_[np.ones((1000, 1)), X_moons]
y_moons = y_moons.reshape(-1, 1)
test_size = 0.2
test_num = int(len(X_moons)*test_size)
X_train = X_moons_with_bias[:-test_num]
y_train = y_moons[:-test_num]
X_test = X_moons_with_bias[-test_num:]
y_test = y_moons[-test_num:]
X_train_enhanced = np.c_[X_train,
                         np.square(X_train[:, 1]),
                         np.square(X_train[:, 2]),
                         X_train[:, 1] ** 3,
                         X_train[:, 2] ** 3]
X_test_enhanced = np.c_[X_test,
                        np.square(X_test[:, 1]),
                        np.square(X_test[:, 2]),
                        X_test[:, 1] ** 3,
                        X_test[:, 2] ** 3]
                        
reset_graph()
def logistic_regression(X, y, initializer=None, seed=42, learning_rate=0.01):
    n_inputs_including_bias = int(X.get_shape()[1])
    with tf.name_scope("logistic_regression"):
        with tf.name_scope("model"):
            if initializer is None:
                initializer = tf.random_uniform([n_inputs_including_bias, 1], -1.0, 1.0, seed=seed)
            theta = tf.Variable(initializer, name="theta")
            logits = tf.matmul(X, theta, name="logits")
            y_proba = tf.sigmoid(logits)
        with tf.name_scope("train"):
            loss = tf.losses.log_loss(y, y_proba, scope="loss")
            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
            training_op = optimizer.minimize(loss)
            loss_summary = tf.summary.scalar('log_loss', loss)
        with tf.name_scope("init"):
            init = tf.global_variables_initializer()
        with tf.name_scope("save"):
            saver = tf.train.Saver()
    return y_proba, loss, training_op, loss_summary, init, saver
from datetime import datetime

def log_dir(prefix=""):
    now = datetime.utcnow().strftime("%Y%m%d%H%M%S")
    root_logdir = "tf_logs"
    if prefix:
        prefix += "-"
    name = prefix + "run-" + now
    return "{}/{}/".format(root_logdir, name)
n_inputs = 2 + 4
logdir = log_dir("logreg")

X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name="X")
y = tf.placeholder(tf.float32, shape=(None, 1), name="y")
y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(X, y)
file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())

n_epochs = 10001
batch_size = 50
n_batches = int(np.ceil(m / batch_size))

checkpoint_path = "/tmp/my_logreg_model.ckpt"
checkpoint_epoch_path = checkpoint_path + ".epoch"
final_model_path = "./my_logreg_model"

with tf.Session() as sess:
    if os.path.isfile(checkpoint_epoch_path):
        # if the checkpoint file exists, restore the model and load the epoch number
        with open(checkpoint_epoch_path, "rb") as f:
            start_epoch = int(f.read())
        print("Training was interrupted. Continuing at epoch", start_epoch)
        saver.restore(sess, checkpoint_path)
    else:
        start_epoch = 0
        sess.run(init)

    for epoch in range(start_epoch, n_epochs):
        for batch_index in range(n_batches):
            X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)
            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
        loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})
        file_writer.add_summary(summary_str, epoch)
        if epoch % 500 == 0:
            print("Epoch:", epoch, "\tLoss:", loss_val)
            saver.save(sess, checkpoint_path)
            with open(checkpoint_epoch_path, "wb") as f:
                f.write(b"%d" % (epoch + 1))

    saver.save(sess, final_model_path)
    y_proba_val = y_proba.eval(feed_dict={X: X_test_enhanced, y: y_test})
    os.remove(checkpoint_epoch_path)

#adding a simple hyperarameters search

from scipy.stats import reciprocal

n_search_iterations = 10

for search_iteration in range(n_search_iterations):
    batch_size = np.random.randint(1, 100)
    learning_rate = reciprocal(0.0001, 0.1).rvs(random_state=search_iteration)

    n_inputs = 2 + 4
    logdir = log_dir("logreg")
    
    print("Iteration", search_iteration)
    print("  logdir:", logdir)
    print("  batch size:", batch_size)
    print("  learning_rate:", learning_rate)
    print("  training: ", end="")

    reset_graph()

    X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name="X")
    y = tf.placeholder(tf.float32, shape=(None, 1), name="y")

    y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(
        X, y, learning_rate=learning_rate)

    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())

    n_epochs = 10001
    n_batches = int(np.ceil(m / batch_size))

    final_model_path = "./my_logreg_model_%d" % search_iteration

    with tf.Session() as sess:
        sess.run(init)

        for epoch in range(n_epochs):
            for batch_index in range(n_batches):
                X_batch, y_batch = random_batch(X_train_enhanced, y_train, batch_size)
                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
            loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test_enhanced, y: y_test})
            file_writer.add_summary(summary_str, epoch)
            if epoch % 500 == 0:
                print(".", end="")

        saver.save(sess, final_model_path)

        print()
        y_proba_val = y_proba.eval(feed_dict={X: X_test_enhanced, y: y_test})
        y_pred = (y_proba_val >= 0.5)
        
        print("  precision:", precision_score(y_test, y_pred))
        print("  recall:", recall_score(y_test, y_pred))











