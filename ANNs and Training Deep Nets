#the perceptron learning rule: w(i, j) = w(i, j) + eta*(y-y(hat))*xi y(hat) is the prediction
'''
   step function: heavside(z): 0 (z<0), 1(z>=0),
   def heaviside(z):
       return (z >= 0).astype(z.dtype)
   sgn(z)
   LTU: a linear threshold unit    MLP: Multi-Layer Perceptron: one input layer | logistic function, 
   hyperbolic tangent function: tanh(z)=2sigmoid(2*z)-1,(-1, 1)   relu function:
   one or more layers of LTUs calle hidden layers(include output layer),
   ANN has two or more layers of hidden layers called DNN
   for each training instances the Backpropagation algotithm(BP) first make predictions(forward pass), measure the error, goes
   through each layer in reverse to measure the error contribution from each connection(reverse pass), and finally slightly tweak
   the connection weights to reduce the error(GD step)
'''
from sklearn.linear_model import Perceptron
per_clf = Perceptron(random_state=42) # equal to sgd_clf = SGDClassifier(loss='perceptron', learning_rate='constant', eta0=1, penality=None)
#using high level API
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0
X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0 # scale
y_train = y_train.astype(np.int32)# from np.uint8 to np.int32
y_test = y_test.astype(np.int32)
feature_cols = [tf.feature_column.numeric_column("X", shape=[28 * 28])]
dnn_clf = tf.estimator.DNNClassifier(hidden_units=[300,100], n_classes=10,
                                     feature_columns=feature_cols)
input_fn = tf.estimator.inputs.numpy_input_fn(
    x={"X": X_train}, y=y_train, num_epochs=40, batch_size=50, shuffle=True)
dnn_clf.train(input_fn=input_fn)
......
test_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={"X": X_test}, y=y_test
    , shuffle=False)
eval_results = dnn_clf.evaluate(input_fn=test_input_fn)
......
y_pred_iter = dnn_clf.predict(input_fn=test_input_fn)
y_pred = list(y_pred_iter)
y_pred[0]
...
correct = tf.nn.in_top_k(logits, y, 1) # if the max k values' index equal to y, return True
accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))

#Xavier initialization(tf.layers.dense default), but when using Relu, ELU, He initialization is recommend;
he_init = tf.variance_scaling_initializer()
hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, kernel_initializer=he_init, name="hidden1")
# define leaky_relu:
def leaky_relu(z, name=None):
    return tf.maximum(0.01 * z, z, name=name)
# the tensorflow has defined yet
def elu(z, alpha=1):
    return np.where(z < 0, alpha * (np.exp(z) - 1), z)
# in general elu > leaky_relu > relu > tanh > logistic
# the selu activation function which can be used as td.nn.selu
def selu(z,
         scale=1.0507009873554804934193349852946,
         alpha=1.6732632423543772848170429916717):
    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))
'''
   During training, a neural network composed of a stack of dense layers using the SELU activation function
   will self-normalize: the output of each layer will tend to preserve the same mean and variance during training,
   which solves the vanishing/exploding gradients problem
'''

