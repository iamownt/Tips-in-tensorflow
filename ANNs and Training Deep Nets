#the perceptron learning rule: w(i, j) = w(i, j) + eta*(y-y(hat))*xi y(hat) is the prediction
'''
   step function: heavside(z): 0 (z<0), 1(z>=0),
   def heaviside(z):
       return (z >= 0).astype(z.dtype)
   sgn(z)
   LTU: a linear threshold unit    MLP: Multi-Layer Perceptron: one input layer | logistic function, 
   hyperbolic tangent function: tanh(z)=2sigmoid(2*z)-1,(-1, 1)   relu function:
   one or more layers of LTUs calle hidden layers(include output layer),
   ANN has two or more layers of hidden layers called DNN
   for each training instances the Backpropagation algotithm(BP) first make predictions(forward pass), measure the error, goes
   through each layer in reverse to measure the error contribution from each connection(reverse pass), and finally slightly tweak
   the connection weights to reduce the error(GD step)
'''
from sklearn.linear_model import Perceptron
per_clf = Perceptron(random_state=42) # equal to sgd_clf = SGDClassifier(loss='perceptron', learning_rate='constant', eta0=1, penality=None)
#using high level API
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0
X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0 # scale
y_train = y_train.astype(np.int32)# from np.uint8 to np.int32
y_test = y_test.astype(np.int32)
feature_cols = [tf.feature_column.numeric_column("X", shape=[28 * 28])]
dnn_clf = tf.estimator.DNNClassifier(hidden_units=[300,100], n_classes=10,
                                     feature_columns=feature_cols)
input_fn = tf.estimator.inputs.numpy_input_fn(
    x={"X": X_train}, y=y_train, num_epochs=40, batch_size=50, shuffle=True)
dnn_clf.train(input_fn=input_fn)
......
test_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={"X": X_test}, y=y_test
    , shuffle=False)
eval_results = dnn_clf.evaluate(input_fn=test_input_fn)
......
y_pred_iter = dnn_clf.predict(input_fn=test_input_fn)
y_pred = list(y_pred_iter)
y_pred[0]
...
correct = tf.nn.in_top_k(logits, y, 1) # if the max k values' index equal to y, return True
accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))

#Xavier initialization(tf.layers.dense default), but when using Relu, ELU, He initialization is recommend;
he_init = tf.variance_scaling_initializer()
hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, kernel_initializer=he_init, name="hidden1")
# define leaky_relu:
def leaky_relu(z, name=None):
    return tf.maximum(0.01 * z, z, name=name)
# the tensorflow has defined yet
def elu(z, alpha=1):
    return np.where(z < 0, alpha * (np.exp(z) - 1), z)
# in general elu > leaky_relu > relu > tanh > logistic
# the selu activation function which can be used as td.nn.selu
def selu(z,
         scale=1.0507009873554804934193349852946,
         alpha=1.6732632423543772848170429916717):
    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))
'''
   During training, a neural network composed of a stack of dense layers using the SELU activation function
   will self-normalize: the output of each layer will tend to preserve the same mean and variance during training,
   which solves the vanishing/exploding gradients problem
'''
!!! but the inputs must be standardized with mean  and standard deviation !!!
X_train = (X_train - X_train.mean(axis=0, keepdims=True))/(X_train.std(axis=0, keepdims=True)+1e-10)
X_test = (X_test - X_train.mean(axis=0, keepdims=True))/(X_train.std(axis=0, keepdims=True)+1e-10)

# using batch_normalization!
from fuctools import partial
reset_graph()
batch_norm_momentum = 0.9
X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")
y = tf.placeholder(tf.int32, shape=(None), name="y")
training = tf.placeholder_with_default(False, shape=(), name='training')
with tf.name_scope("dnn"):
    he_init = tf.variance_scaling_initializer()
    my_batch_norm_layer = partial(
            tf.layers.batch_normalization,
            training=training,
            momentum=batch_norm_momentum)
    my_dense_layer = partial(
            tf.layers.dense,
            kernel_initializer=he_init)
    hidden1 = my_dense_layer(X, n_hidden1, name="hidden1")
    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))
    hidden2 = my_dense_layer(bn1, n_hidden2, name="hidden2")
    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))
    logits_before_bn = my_dense_layer(bn2, n_outputs, name="outputs")
    logits = my_batch_norm_layer(logits_before_bn)
with tf.name_scope("loss"):
    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)
    loss = tf.reduce_mean(xentropy, name="loss")
with tf.name_scope("train"):
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    training_op = optimizer.minimize(loss)
with tf.name_scope("eval"):
    correct = tf.nn.in_top_k(logits, y, 1)
    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))
init = tf.global_variables_initializer()
saver = tf.train.Saver()
n_epochs = 20
batch_size = 200
extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)#pay attention to extra_update_ops and sess.run([..., extra_update_ops],.!!)
with tf.Session() as sess:
    init.run()
    for epoch in range(n_epochs):
        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):
            sess.run([training_op, extra_update_ops],
                     feed_dict={training: True, X: X_batch, y: y_batch})
        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})
        print(epoch, "Validation accuracy:", accuracy_val)
    save_path = saver.save(sess, "./my_model_final.ckpt")
# to see the trainable_variable and global_varibale:
[v.name for v in tf.trainable_variables()]
[v.name for v in tf.global_variables()]
# Gradient Clipping
threshold = 1.0
optimizer = tf.train.GradientDescentOptimizer(learning_rate)
grads_and_vars = optimizer.compute_gradients(loss)
capped_gvs = [(tf.clip_by_value(grads, -threshold, threshold), vars) for grads, vars in grads_and_vars]
training_op = optimizer.apply_gradients(capped_gvs)
#Batch Normalization acts like  a regularizer reducing the need for other regularization techniques(dropout)
#Batch Normalization find the optimal scale and offset for each layer

#Reusing the pretrained layers
1. reusing a graph structure using:
   saver = tf.train.import_meta_graph('./my_final_model.ckpt.meta')
   #Next, need to get a handle on all the operations for training. If don't know the graph's structure, list all the operations:
   for op in tf.get_default_graph().get_operations():
       print(op.name)
   X = tf.get_default_graph().get_tensor_by_name("X:0")
   y = tf.get_default_graph().get_tensor_by_name("y:0")
   accuracy = tf.get_default_graph().get_tensor_by_name("eval/accuracy:0")
   training_op = tf.get_default_graph().get_operation_by_name("GradientDescent")
   #we could make things easier for people who will reuse your model by giving operations very clear names and documenting them
   '''like:'''
   for op in (X, y, accuracy, training_op):
       tf.add_to_collection("my_important_ops", op)
   X, y, accuracy, training_op = tf.get_collection("my_important_ops") 
!!!we use 'saver = tf.train.import_meta_graph('./...')' to create the graph(operation), if we also want to evaluate something
   such as accuracy, we must using X = tf.get_default_graph().get_tensor_by_name....... if using in execution phase and
   with tf.Session() as sess:
       saver.restore(sess, '......ckpt') just restore the parameters
    if we have 'the construction phase code' we do not need to using like 'get_tensor_by_name'......
!!!
# using a new 4th layer and 3 pretrained layers:
reset_graph()
n_hidden4 = 20  # new layer
n_outputs = 10  # new layer
saver = tf.train.import_meta_graph("./my_model_final.ckpt.meta")
X = tf.get_default_graph().get_tensor_by_name("X:0")
y = tf.get_default_graph().get_tensor_by_name("y:0")
hidden3 = tf.get_default_graph().get_tensor_by_name("dnn/hidden3/Relu:0")
new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name="new_hidden4")
new_logits = tf.layers.dense(new_hidden4, n_outputs, name="new_outputs")
with tf.name_scope("new_loss"):
    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)
    loss = tf.reduce_mean(xentropy, name="loss")
with tf.name_scope("new_eval"):
    correct = tf.nn.in_top_k(new_logits, y, 1)
    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name="accuracy")
with tf.name_scope("new_train"):
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    training_op = optimizer.minimize(loss)
init = tf.global_variables_initializer()
new_saver = tf.train.Saver()
with tf.Session() as sess:
    init.run()
    saver.restore(sess, "./my_model_final.ckpt")
    for epoch in range(n_epochs):
        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):
            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})
        print(epoch, "Validation accuracy:", accuracy_val)
    save_path = new_saver.save(sess, "./my_new_model_final.ckpt")

#also if using the construction phase, must give the list of variables to restore!!
reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,
                               scope="hidden[123]") # regular expression
restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3
init = tf.global_variables_initializer()
saver = tf.train.Saver()
with tf.Session() as sess:
    init.run()
    restore_saver.restore(sess, "./my_model_final.ckpt")

    for epoch in range(n_epochs):                                            
        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): 
            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})        
        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})     
        print(epoch, "Validation accuracy:", accuracy_val)                   

    save_path = saver.save(sess, "./my_new_model_final.ckpt")




















